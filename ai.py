import os
import random
import threading
import time
from collections import deque, Counter
import numpy as np
import joblib # S·ª≠ d·ª•ng joblib ƒë·ªÉ l∆∞u/t·∫£i model sklearn v√† scaler

# --- C√†i ƒë·∫∑t th∆∞ vi·ªán ---
# ƒê·∫£m b·∫£o c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t
try:
    import sklearn
    print(f"Scikit-learn version: {sklearn.__version__}")
except ImportError:
    print("ƒêang c√†i ƒë·∫∑t scikit-learn...")
    os.system("pip install scikit-learn --upgrade")
try:
    import tensorflow as tf
    print(f"TensorFlow version: {tf.__version__}")
    # Ki·ªÉm tra GPU (t√πy ch·ªçn)
    # gpus = tf.config.list_physical_devices('GPU')
    # if gpus:
    #     print(f"TensorFlow ƒëang s·ª≠ d·ª•ng GPU: {gpus}")
    # else:
    #     print("TensorFlow ƒëang s·ª≠ d·ª•ng CPU.")
except ImportError:
    print("ƒêang c√†i ƒë·∫∑t tensorflow...")
    os.system("pip install tensorflow --upgrade")

# Import sau khi ƒë·∫£m b·∫£o ƒë√£ c√†i ƒë·∫∑t
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier # Th√™m Gradient Boosting
from sklearn.preprocessing import StandardScaler, MinMaxScaler # S·ª≠ d·ª•ng StandardScaler
from sklearn.exceptions import NotFittedError

from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional # Th√™m BatchNormalization, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau # Th√™m callbacks h·ªØu √≠ch
from tensorflow.keras.regularizers import l1_l2 # Th√™m regularizers

from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, constants
from telegram.ext import ApplicationBuilder, CommandHandler, CallbackQueryHandler, ContextTypes, MessageHandler, filters
from telegram.helpers import escape_markdown

print("C√°c th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c import th√†nh c√¥ng!")

# --- C·∫•u h√¨nh v√† H·∫±ng s·ªë ---
# Token bot Telegram (L·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng)
TOKEN = os.getenv("TELEGRAM_TOKEN", "YOUR_TELEGRAM_BOT_TOKEN") # Th√™m token m·∫∑c ƒë·ªãnh ƒë·ªÉ d·ªÖ test h∆°n
if TOKEN == "YOUR_TELEGRAM_BOT_TOKEN":
    print("C·∫£nh b√°o: S·ª≠ d·ª•ng token m·∫∑c ƒë·ªãnh. Vui l√≤ng ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng TELEGRAM_TOKEN!")

# --- Qu·∫£n l√Ω D·ªØ li·ªáu v√† Model ---
HISTORY_MAXLEN = 1000 # TƒÉng gi·ªõi h·∫°n l·ªãch s·ª≠
DICE_MAXLEN = 1000    # L∆∞u ƒëi·ªÉm t·ªïng c·ªßa s√∫c s·∫Øc (hi·ªán √≠t d√πng cho model)
MIN_HISTORY_FOR_TRAIN = 50  # S·ªë l∆∞·ª£ng b·∫£n ghi l·ªãch s·ª≠ t·ªëi thi·ªÉu ƒë·ªÉ b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán
MIN_HISTORY_FOR_PREDICT = 20 # S·ªë l∆∞·ª£ng b·∫£n ghi l·ªãch s·ª≠ t·ªëi thi·ªÉu ƒë·ªÉ d·ª± ƒëo√°n
LSTM_SEQUENCE_LENGTH = 25 # ƒê·ªô d√†i chu·ªói cho LSTM (quan tr·ªçng!)

MODEL_DIR = "advanced_models" # Th∆∞ m·ª•c l∆∞u model m·ªõi
os.makedirs(MODEL_DIR, exist_ok=True) # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥

# ƒê∆∞·ªùng d·∫´n file
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)
HISTORY_FILE = os.path.join(DATA_DIR,"history_data.npy")
DICE_FILE = os.path.join(DATA_DIR,"dice_data.npy")
SCALER_SKLEARN_FILE = os.path.join(MODEL_DIR, "sklearn_scaler.joblib")
NB_MODEL_FILE = os.path.join(MODEL_DIR, "nb_model.joblib")
LR_MODEL_FILE = os.path.join(MODEL_DIR, "lr_model.joblib")
RF_MODEL_FILE = os.path.join(MODEL_DIR, "rf_model.joblib")
GB_MODEL_FILE = os.path.join(MODEL_DIR, "gb_model.joblib") # File cho Gradient Boosting
LSTM_MODEL_FILE = os.path.join(MODEL_DIR, "adv_lstm_best_model.keras") # S·ª≠ d·ª•ng .keras cho model TF

# --- Bi·∫øn to√†n c·ª•c ---
history_data = deque(maxlen=HISTORY_MAXLEN)
dice_data = deque(maxlen=DICE_MAXLEN)
sklearn_scaler = StandardScaler() # S·ª≠ d·ª•ng StandardScaler cho Sklearn features
nb_model = GaussianNB()
lr_model = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced')
rf_model = RandomForestClassifier(n_estimators=150, random_state=42, class_weight='balanced', max_depth=10, min_samples_leaf=5) # Tinh ch·ªânh RF
gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42) # Th√™m GB
lstm_model = None # S·∫Ω load ho·∫∑c build sau

# Callbacks cho vi·ªác hu·∫•n luy·ªán LSTM
# L∆∞u model t·ªët nh·∫•t d·ª±a tr√™n validation loss (n·∫øu c√≥ validation split) ho·∫∑c loss
lstm_checkpoint = ModelCheckpoint(
    LSTM_MODEL_FILE, monitor="val_loss" if MIN_HISTORY_FOR_TRAIN > 50 else "loss", save_best_only=True,
    verbose=1, mode='min', save_weights_only=False # L∆∞u c·∫£ ki·∫øn tr√∫c
)
# D·ª´ng s·ªõm n·∫øu validation loss kh√¥ng c·∫£i thi·ªán
early_stopping = EarlyStopping(
    monitor="val_loss" if MIN_HISTORY_FOR_TRAIN > 50 else "loss", patience=15, # TƒÉng patience
    verbose=1, mode='min', restore_best_weights=True # Kh√¥i ph·ª•c tr·ªçng s·ªë t·ªët nh·∫•t
)
# Gi·∫£m learning rate n·∫øu kh√¥ng c·∫£i thi·ªán
reduce_lr = ReduceLROnPlateau(
    monitor="val_loss" if MIN_HISTORY_FOR_TRAIN > 50 else "loss", factor=0.2, patience=7, # TƒÉng patience
    verbose=1, mode='min', min_lr=1e-6 # LR t·ªëi thi·ªÉu
)

# C·ªù tr·∫°ng th√°i models
sklearn_models_ready = False
lstm_model_ready = False
training_active = False
training_lock = threading.Lock()

# --- Qu·∫£n l√Ω L∆∞u/T·∫£i D·ªØ li·ªáu v√† Model ---
def save_all_data_models():
    """L∆∞u l·ªãch s·ª≠, s√∫c s·∫Øc, scaler v√† t·∫•t c·∫£ c√°c m√¥ h√¨nh."""
    print("--- B·∫Øt ƒë·∫ßu L∆∞u D·ªØ li·ªáu & Models ---")
    try:
        np.save(HISTORY_FILE, np.array(history_data))
        # np.save(DICE_FILE, np.array(dice_data)) # L∆∞u n·∫øu c·∫ßn
        print(f"ƒê√£ l∆∞u {len(history_data)} m·ª•c l·ªãch s·ª≠.")

        # L∆∞u models sklearn v√† scaler
        joblib.dump(sklearn_scaler, SCALER_SKLEARN_FILE)
        print("Scaler Sklearn ƒë√£ l∆∞u.")
        if sklearn_models_ready:
            joblib.dump(nb_model, NB_MODEL_FILE)
            joblib.dump(lr_model, LR_MODEL_FILE)
            joblib.dump(rf_model, RF_MODEL_FILE)
            joblib.dump(gb_model, GB_MODEL_FILE) # L∆∞u GB
            print("C√°c m√¥ h√¨nh Sklearn ƒë√£ l∆∞u.")
        # L∆∞u LSTM model (Keras t·ª± x·ª≠ l√Ω trong ModelCheckpoint, nh∆∞ng c√≥ th·ªÉ save l·∫ßn cu·ªëi ·ªü ƒë√¢y)
        # if lstm_model_ready and lstm_model:
        #    lstm_model.save(LSTM_MODEL_FILE) # ƒê·∫£m b·∫£o model m·ªõi nh·∫•t ƒë∆∞·ª£c l∆∞u (c√≥ th·ªÉ ghi ƒë√® checkpoint)
        #    print("LSTM model ƒë√£ ƒë∆∞·ª£c l∆∞u (l·∫ßn cu·ªëi).")

    except Exception as e:
        print(f"L·ªñI khi l∆∞u d·ªØ li·ªáu/models: {e}")
    finally:
         print("--- K·∫øt th√∫c L∆∞u D·ªØ li·ªáu & Models ---")

def load_all_data_models():
    """T·∫£i l·ªãch s·ª≠, s√∫c s·∫Øc, scaler v√† t·∫•t c·∫£ c√°c m√¥ h√¨nh."""
    global history_data, dice_data, sklearn_scaler, nb_model, lr_model, rf_model, gb_model, lstm_model, sklearn_models_ready, lstm_model_ready
    print("--- B·∫Øt ƒë·∫ßu T·∫£i D·ªØ li·ªáu & Models ---")
    models_loaded_count = 0
    try:
        # T·∫£i d·ªØ li·ªáu
        if os.path.exists(HISTORY_FILE):
            loaded_history = np.load(HISTORY_FILE).tolist()
            # Ch·ªâ th√™m v√†o deque n·∫øu n√≥ ch∆∞a t·ªìn t·∫°i ƒë·ªÉ tr√°nh tr√πng l·∫∑p khi kh·ªüi ƒë·ªông l·∫°i nhanh
            current_set = set(list(history_data)[-len(loaded_history):]) if history_data else set()
            new_items = [item for item in loaded_history if tuple(item) not in current_set] # Gi·∫£ s·ª≠ history_data l∆∞u tuple ho·∫∑c str
            history_data.extend(new_items) # D√πng extend
            print(f"ƒê√£ t·∫£i {len(loaded_history)} m·ª•c l·ªãch s·ª≠ (th√™m {len(new_items)} m·ªõi).")
        # if os.path.exists(DICE_FILE): # T·∫£i n·∫øu c·∫ßn
        #     loaded_dice = np.load(DICE_FILE).tolist()
        #     dice_data.extend(loaded_dice) # D√πng extend
        #     print(f"ƒê√£ t·∫£i {len(loaded_dice)} m·ª•c s√∫c s·∫Øc.")

        # T·∫£i scaler v√† models sklearn
        if os.path.exists(SCALER_SKLEARN_FILE):
            sklearn_scaler = joblib.load(SCALER_SKLEARN_FILE)
            print("Scaler Sklearn ƒë√£ t·∫£i.")
        if os.path.exists(NB_MODEL_FILE):
            nb_model = joblib.load(NB_MODEL_FILE)
            models_loaded_count+=1
        if os.path.exists(LR_MODEL_FILE):
            lr_model = joblib.load(LR_MODEL_FILE)
            models_loaded_count+=1
        if os.path.exists(RF_MODEL_FILE):
            rf_model = joblib.load(RF_MODEL_FILE)
            models_loaded_count+=1
        if os.path.exists(GB_MODEL_FILE): # T·∫£i GB
             gb_model = joblib.load(GB_MODEL_FILE)
             models_loaded_count+=1

        if models_loaded_count == 4: # Check all 4 sklearn models
             sklearn_models_ready = True
             print(f"ƒê√£ t·∫£i {models_loaded_count}/4 m√¥ h√¨nh Sklearn.")
        else:
             print(f"Ch∆∞a ƒë·ªß m√¥ h√¨nh Sklearn ({models_loaded_count}/4). C·∫ßn hu·∫•n luy·ªán l·∫°i.")

        # T·∫£i model LSTM
        if os.path.exists(LSTM_MODEL_FILE):
            try:
                # ƒê·∫∑t custom_objects n·∫øu b·∫°n c√≥ l·ªõp t√πy ch·ªânh (·ªü ƒë√¢y kh√¥ng c·∫ßn)
                lstm_model = load_model(LSTM_MODEL_FILE)
                lstm_model_ready = True
                print("LSTM model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng.")
            except Exception as e:
                print(f"L·ªói khi t·∫£i LSTM model t·ª´ '{LSTM_MODEL_FILE}': {e}. C·∫ßn hu·∫•n luy·ªán l·∫°i.")
                lstm_model_ready = False
        else:
            print(f"Kh√¥ng t√¨m th·∫•y file LSTM model t·∫°i '{LSTM_MODEL_FILE}'. C·∫ßn hu·∫•n luy·ªán.")
            lstm_model_ready = False

    except FileNotFoundError:
        print("Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu/model c≈©. B·∫Øt ƒë·∫ßu v·ªõi d·ªØ li·ªáu/model m·ªõi.")
    except Exception as e:
        print(f"L·ªñI nghi√™m tr·ªçng khi t·∫£i d·ªØ li·ªáu/model: {e}")
    finally:
         print(f"--- K·∫øt th√∫c T·∫£i D·ªØ li·ªáu & Models --- (Sklearn Ready: {sklearn_models_ready}, LSTM Ready: {lstm_model_ready})")


# --- Feature Engineering ---
def get_basic_patterns(history_window):
    """ Tr√≠ch xu·∫•t th√¥ng tin v·ªÅ c·∫ßu c∆° b·∫£n t·ª´ c·ª≠a s·ªï l·ªãch s·ª≠ """
    n = len(history_window)
    if n < 3: return 0, 0, 0 # b·ªát, 1-1, 2-2

    # B·ªát
    bet_count = 0
    last = history_window[-1]
    for i in range(n - 1, -1, -1):
        if history_window[i] == last: bet_count += 1
        else: break

    # 1-1
    one_one_count = 0
    if n >= 2 and history_window[-1] != history_window[-2]:
       one_one_count = 2
       for i in range(n - 3, -1, -2):
          if i >= 0 and history_window[i] != history_window[i+1] and history_window[i] == history_window[i+2] :
               one_one_count += 2
          else:
              break
    elif n >= 4 and all(history_window[i] != history_window[i+1] for i in range(-4, -1)): # TƒÉng c∆∞·ªùng check 1-1
        one_one_count = 4

    # 2-2
    two_two_count = 0
    if n >= 4:
        p1 = history_window[-4:-2]
        p2 = history_window[-2:]
        if p1[0] == p1[1] and p2[0] == p2[1] and p1[0] != p2[0]:
            two_two_count = 4
            if n >= 6:
                 p0 = history_window[-6:-4]
                 if p0[0] == p0[1] and p0[0] != p1[0]:
                     two_two_count = 6 # Ch·ªâ t√≠nh 2-2-2... ƒë∆°n gi·∫£n

    return bet_count, one_one_count // 2, two_two_count // 2 # Tr·∫£ v·ªÅ ƒë·ªô d√†i/s·ªë c·∫∑p


def create_sklearn_features(history, lookback=15):
    """
    T·∫°o features ph·ª©c t·∫°p h∆°n cho Sklearn models.
    S·ª≠ d·ª•ng m·ªôt c·ª≠a s·ªï l·ªãch s·ª≠ (`history`) ƒë·ªÉ t·∫°o features cho ƒëi·ªÉm *sau* c·ª≠a s·ªï ƒë√≥.
    """
    n = len(history)
    if n < lookback:
        return None # Kh√¥ng ƒë·ªß d·ªØ li·ªáu trong c·ª≠a s·ªï

    window = history[-lookback:]
    features = []

    # 1. Lag features (3 k·∫øt qu·∫£ cu·ªëi)
    for i in range(1, 4):
        features.append(1 if window[-i] == 't' else 0)

    # 2. T·ª∑ l·ªá T/X trong c·ª≠a s·ªï
    tai_count = window.count('t')
    xiu_count = lookback - tai_count
    features.append(tai_count / lookback)
    features.append(xiu_count / lookback)

    # 3. C√°c ch·ªâ s·ªë C·∫ßu (B·ªát, 1-1, 2-2)
    bet, one_one, two_two = get_basic_patterns(window)
    features.append(bet)
    features.append(one_one)
    features.append(two_two)

    # 4. Thay ƒë·ªïi t·ª∑ l·ªá T/X (so s√°nh 5 cu·ªëi vs 10 tr∆∞·ªõc ƒë√≥ trong c·ª≠a s·ªï)
    if lookback >= 15:
       ratio_last_5 = window[-5:].count('t') / 5
       ratio_prev_10 = window[-15:-5].count('t') / 10 if n >= 15 else 0
       features.append(ratio_last_5 - ratio_prev_10)
    else:
        features.append(0) # Gi√° tr·ªã m·∫∑c ƒë·ªãnh n·∫øu lookback nh·ªè

    # 5. T·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa c√°c b·ªô 2 v√† b·ªô 3 g·∫ßn nh·∫•t
    if n >= 2:
       last_pair = "".join(window[-2:])
       pair_counts = Counter("".join(window[i:i+2]) for i in range(lookback-1))
       features.append(pair_counts.get(last_pair, 0) / (lookback-1))
    else: features.append(0)
    if n >= 3:
        last_triplet = "".join(window[-3:])
        triplet_counts = Counter("".join(window[i:i+3]) for i in range(lookback-2))
        features.append(triplet_counts.get(last_triplet, 0) / (lookback-2))
    else: features.append(0)


    return np.array(features) # Tr·∫£ v·ªÅ 1D array

def prepare_training_data_sklearn(full_history, lookback=15):
    """Chu·∫©n b·ªã X, y cho vi·ªác hu·∫•n luy·ªán Sklearn."""
    X, y = [], []
    labels = [1 if result == 't' else 0 for result in full_history]

    # C·∫ßn √≠t nh·∫•t lookback + 1 ƒëi·ªÉm ƒë·ªÉ t·∫°o feature ƒë·∫ßu ti√™n v√† c√≥ nh√£n
    if len(full_history) < lookback + 1:
        return None, None

    for i in range(lookback, len(full_history)):
        history_window = full_history[i - lookback : i]
        features = create_sklearn_features(history_window, lookback)
        if features is not None:
            X.append(features)
            y.append(labels[i]) # Nh√£n l√† k·∫øt qu·∫£ t·∫°i th·ªùi ƒëi·ªÉm i

    if not X: return None, None
    return np.array(X), np.array(y)


def prepare_lstm_data(full_history, sequence_length=LSTM_SEQUENCE_LENGTH):
    """Chu·∫©n b·ªã d·ªØ li·ªáu (sequence_length) -> 1 cho LSTM."""
    X, y = [], []
    # Ch·ªâ s·ª≠ d·ª•ng 0 v√† 1 cho LSTM inputs
    labels = [1.0 if result == 't' else 0.0 for result in full_history]

    if len(labels) <= sequence_length:
        return None, None

    for i in range(len(labels) - sequence_length):
        X.append(labels[i:i + sequence_length])
        y.append(labels[i + sequence_length])

    if not X: return None, None

    X = np.array(X).astype(np.float32) # Chuy·ªÉn sang float32
    y = np.array(y).astype(np.float32)
    # Reshape X cho LSTM: [samples, time_steps, features=1]
    X = X.reshape((X.shape[0], X.shape[1], 1))
    return X, y

# --- X√¢y d·ª±ng M√¥ h√¨nh LSTM N√¢ng cao ---
def build_advanced_lstm_model(input_shape):
    """X√¢y d·ª±ng ki·∫øn tr√∫c LSTM ph·ª©c t·∫°p h∆°n."""
    model = Sequential(name="Advanced_LSTM")
    # L·ªõp 1: Bidirectional LSTM - H·ªçc chu·ªói theo c·∫£ hai chi·ªÅu
    model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape))
    model.add(BatchNormalization()) # Chu·∫©n h√≥a batch
    model.add(Dropout(0.4)) # Dropout m·∫°nh h∆°n

    # L·ªõp 2: LSTM th√¥ng th∆∞·ªùng
    model.add(LSTM(96, return_sequences=True)) # Gi·∫£m s·ªë units m·ªôt ch√∫t
    model.add(BatchNormalization())
    model.add(Dropout(0.3))

    # L·ªõp 3: LSTM cu·ªëi c√πng
    model.add(LSTM(64, return_sequences=False)) # return_sequences=False tr∆∞·ªõc Dense
    model.add(BatchNormalization())
    model.add(Dropout(0.3))

    # L·ªõp Dense trung gian
    model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))) # Th√™m ReLU v√† Regularization

    # L·ªõp Output
    model.add(Dense(1, activation='sigmoid')) # Sigmoid cho d·ª± ƒëo√°n x√°c su·∫•t T√†i/X·ªâu

    # Compile v·ªõi Adam optimizer v√† learning rate t√πy ch·ªânh
    optimizer = Adam(learning_rate=0.001) # B·∫Øt ƒë·∫ßu v·ªõi LR ph·ªï bi·∫øn
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    print("--- Advanced LSTM Model Summary ---")
    model.summary()
    print("---------------------------------")
    return model


# --- Hu·∫•n luy·ªán M√¥ h√¨nh ---
def train_all_models(force_train=False):
    """Hu·∫•n luy·ªán t·∫•t c·∫£ c√°c m√¥ h√¨nh n·∫øu ƒë·ªß d·ªØ li·ªáu ho·∫∑c b·ªã √©p."""
    global sklearn_models_ready, lstm_model_ready, lstm_model, sklearn_scaler, training_active

    # Ch·ªâ ch·∫°y n·∫øu kh√¥ng c√≥ hu·∫•n luy·ªán n√†o ƒëang di·ªÖn ra
    if training_lock.locked() and not force_train:
        print("Hu·∫•n luy·ªán b·ªã b·ªè qua v√¨ ƒë√£ c√≥ ti·∫øn tr√¨nh kh√°c ƒëang ch·∫°y.")
        return
    with training_lock:
        training_active = True
        print("\n=== B·∫ÆT ƒê·∫¶U QU√Å TR√åNH HU·∫§N LUY·ªÜN ===")
        current_full_history = list(history_data)
        n_history = len(current_full_history)
        print(f"S·ªë l∆∞·ª£ng l·ªãch s·ª≠ hi·ªán t·∫°i: {n_history}")

        training_performed = False # C·ªù ƒë·ªÉ bi·∫øt c√≥ l∆∞u model kh√¥ng

        # --- Hu·∫•n luy·ªán Sklearn Models ---
        sklearn_lookback = 15 # Ph·∫£i kh·ªõp v·ªõi `create_sklearn_features`
        min_data_sklearn = max(MIN_HISTORY_FOR_TRAIN, sklearn_lookback + 5) # C·∫ßn ƒë·ªß ƒë·ªÉ t·∫°o v√†i sample
        if n_history >= min_data_sklearn:
            print(f"\n[SKLEARN] ƒê·ªß d·ªØ li·ªáu ({n_history}/{min_data_sklearn}), b·∫Øt ƒë·∫ßu chu·∫©n b·ªã...")
            X_sk, y_sk = prepare_training_data_sklearn(current_full_history, lookback=sklearn_lookback)

            if X_sk is not None and y_sk is not None and len(X_sk) >= 10: # C·∫ßn √≠t nh·∫•t 10 m·∫´u ƒë·ªÉ train ·ªïn
                print(f"[SKLEARN] Chu·∫©n b·ªã {X_sk.shape[0]} m·∫´u (features: {X_sk.shape[1]}). B·∫Øt ƒë·∫ßu Scaling v√† Training...")
                try:
                    sklearn_scaler.fit(X_sk) # Fit scaler ch·ªâ tr√™n d·ªØ li·ªáu train
                    X_scaled = sklearn_scaler.transform(X_sk)
                    print("[SKLEARN] Scaling ho√†n th√†nh.")

                    # Train models
                    print("[SKLEARN] Training Naive Bayes...")
                    nb_model.fit(X_scaled, y_sk)
                    print("[SKLEARN] Training Logistic Regression...")
                    lr_model.fit(X_scaled, y_sk)
                    print("[SKLEARN] Training Random Forest...")
                    rf_model.fit(X_scaled, y_sk)
                    print("[SKLEARN] Training Gradient Boosting...")
                    gb_model.fit(X_scaled, y_sk) # Train GB

                    sklearn_models_ready = True
                    training_performed = True
                    print("‚úÖ [SKLEARN] Hu·∫•n luy·ªán th√†nh c√¥ng!")

                except Exception as e:
                    print(f"‚ùå [SKLEARN] L·ªñI trong qu√° tr√¨nh hu·∫•n luy·ªán: {e}")
                    sklearn_models_ready = False # ƒê·∫∑t l·∫°i c·ªù n·∫øu l·ªói
            else:
                print("[SKLEARN] Kh√¥ng ƒë·ªß d·ªØ li·ªáu h·ª£p l·ªá sau khi t·∫°o features.")
        else:
            print(f"[SKLEARN] Kh√¥ng ƒë·ªß d·ªØ li·ªáu ({n_history}/{min_data_sklearn}). B·ªè qua hu·∫•n luy·ªán.")

        # --- Hu·∫•n luy·ªán LSTM Model ---
        min_data_lstm = max(MIN_HISTORY_FOR_TRAIN, LSTM_SEQUENCE_LENGTH + 10) # C·∫ßn seq_len + ƒë·ªß sample
        if n_history >= min_data_lstm:
            print(f"\n[LSTM] ƒê·ªß d·ªØ li·ªáu ({n_history}/{min_data_lstm}), b·∫Øt ƒë·∫ßu chu·∫©n b·ªã...")
            X_lstm, y_lstm = prepare_lstm_data(current_full_history, sequence_length=LSTM_SEQUENCE_LENGTH)

            if X_lstm is not None and y_lstm is not None and len(X_lstm) >= 10:
                print(f"[LSTM] Chu·∫©n b·ªã {X_lstm.shape[0]} chu·ªói (d√†i {X_lstm.shape[1]}).")
                try:
                    if lstm_model is None or not isinstance(lstm_model, tf.keras.Model) or force_train:
                        print("[LSTM] X√¢y d·ª±ng ho·∫∑c x√¢y d·ª±ng l·∫°i m√¥ h√¨nh LSTM...")
                        # Input shape l√† (sequence_length, num_features=1)
                        input_shape = (X_lstm.shape[1], X_lstm.shape[2])
                        lstm_model = build_advanced_lstm_model(input_shape=input_shape)
                    else:
                        print("[LSTM] S·ª≠ d·ª•ng l·∫°i m√¥ h√¨nh LSTM ƒë√£ c√≥.")


                    print("[LSTM] B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán (fit)...")
                    # Chia train/validation n·∫øu c√≥ ƒë·ªß d·ªØ li·ªáu (v√≠ d·ª• > 100 samples)
                    validation_split_ratio = 0.15 if len(X_lstm) > 100 else 0.0
                    history = lstm_model.fit(
                        X_lstm, y_lstm,
                        epochs=75, # TƒÉng epochs, EarlyStopping s·∫Ω qu·∫£n l√Ω
                        batch_size=16, # Gi·∫£m batch size
                        validation_split=validation_split_ratio,
                        callbacks=[lstm_checkpoint, early_stopping, reduce_lr],
                        verbose=1, # Hi·ªÉn th·ªã chi ti·∫øt qu√° tr√¨nh hu·∫•n luy·ªán
                        shuffle=True
                    )
                    print("[LSTM] Hu·∫•n luy·ªán (fit) ho√†n th√†nh.")
                    # Sau khi fit xong, model t·ªët nh·∫•t (theo val_loss/loss) ƒë√£ ƒë∆∞·ª£c kh√¥i ph·ª•c b·ªüi EarlyStopping ho·∫∑c l∆∞u b·ªüi Checkpoint
                    # Load l·∫°i model t·ªët nh·∫•t t·ª´ checkpoint ƒë·ªÉ ƒë·∫£m b·∫£o ƒëang d√πng b·∫£n t·ªët nh·∫•t
                    if os.path.exists(LSTM_MODEL_FILE):
                         print("[LSTM] T·∫£i l·∫°i tr·ªçng s·ªë t·ªët nh·∫•t t·ª´ Checkpoint...")
                         lstm_model = load_model(LSTM_MODEL_FILE) # T·∫£i l·∫°i ho√†n to√†n model t·ª´ file t·ªët nh·∫•t
                         lstm_model_ready = True
                         training_performed = True
                         print("‚úÖ [LSTM] Hu·∫•n luy·ªán th√†nh c√¥ng v√† ƒë√£ t·∫£i tr·ªçng s·ªë t·ªët nh·∫•t!")
                    else:
                         print("[LSTM] WARN: Kh√¥ng t√¨m th·∫•y file checkpoint sau khi hu·∫•n luy·ªán? Model c√≥ th·ªÉ ch∆∞a ph·∫£i t·ªët nh·∫•t.")
                         lstm_model_ready = True # V·∫´n coi l√† ready nh∆∞ng c·∫£nh b√°o
                         training_performed = True


                except Exception as e:
                    print(f"‚ùå [LSTM] L·ªñI nghi√™m tr·ªçng trong qu√° tr√¨nh hu·∫•n luy·ªán: {e}")
                    lstm_model_ready = False # ƒê·∫∑t l·∫°i c·ªù n·∫øu l·ªói
            else:
                print("[LSTM] Kh√¥ng ƒë·ªß d·ªØ li·ªáu h·ª£p l·ªá sau khi t·∫°o sequences.")
        else:
            print(f"[LSTM] Kh√¥ng ƒë·ªß d·ªØ li·ªáu ({n_history}/{min_data_lstm}). B·ªè qua hu·∫•n luy·ªán.")

        # L∆∞u t·∫•t c·∫£ models v√† d·ªØ li·ªáu sau khi hu·∫•n luy·ªán (n·∫øu c√≥ thay ƒë·ªïi)
        if training_performed:
            print("\nHo√†n th√†nh v√≤ng hu·∫•n luy·ªán, l∆∞u tr·∫°ng th√°i...")
            save_all_data_models()
        else:
            print("\nKh√¥ng c√≥ hu·∫•n luy·ªán n√†o ƒë∆∞·ª£c th·ª±c hi·ªán trong v√≤ng n√†y.")

        training_active = False
        print("=== K·∫æT TH√öC QU√Å TR√åNH HU·∫§N LUY·ªÜN ===")


# --- D·ª± ƒëo√°n K·∫øt h·ª£p (Ensemble) ---
def combined_prediction(current_history):
    """
    K·∫øt h·ª£p d·ª± ƒëo√°n t·ª´ c√°c m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán.
    S·ª≠ d·ª•ng weighted averaging ho·∫∑c voting d·ª±a tr√™n s·ª± s·∫µn s√†ng c·ªßa models.
    """
    n_hist = len(current_history)
    print(f"\n--- B·∫Øt ƒë·∫ßu D·ª± ƒëo√°n K·∫øt h·ª£p (L·ªãch s·ª≠: {n_hist} m·ª•c) ---")

    if n_hist < MIN_HISTORY_FOR_PREDICT:
        print(f"WARN: L·ªãch s·ª≠ qu√° ng·∫Øn ({n_hist}/{MIN_HISTORY_FOR_PREDICT}). D·ª± ƒëo√°n ng·∫´u nhi√™n.")
        pred = random.choice(["t", "x"])
        return pred, 50.0, 50.0 # Prediction, Prob T, Prob X

    probs_t = {} # L∆∞u tr·ªØ x√°c su·∫•t T√†i t·ª´ m·ªói model

    # --- D·ª± ƒëo√°n t·ª´ Sklearn Models ---
    sklearn_lookback = 15 # Ph·∫£i kh·ªõp v·ªõi l√∫c train
    if sklearn_models_ready and n_hist >= sklearn_lookback:
        print("[Predict SKLEARN] B·∫Øt ƒë·∫ßu...")
        features_sk = create_sklearn_features(current_history, lookback=sklearn_lookback)
        if features_sk is not None:
            try:
                # Reshape v·ªÅ 2D array n·∫øu `create_sklearn_features` tr·∫£ v·ªÅ 1D
                if features_sk.ndim == 1:
                    features_sk = features_sk.reshape(1, -1)

                scaled_features = sklearn_scaler.transform(features_sk)

                probs_t['nb'] = nb_model.predict_proba(scaled_features)[0][1]
                probs_t['lr'] = lr_model.predict_proba(scaled_features)[0][1]
                probs_t['rf'] = rf_model.predict_proba(scaled_features)[0][1]
                probs_t['gb'] = gb_model.predict_proba(scaled_features)[0][1] # L·∫•y prob t·ª´ GB
                print(f"[Predict SKLEARN] Probs (T): "
                      f"NB={probs_t.get('nb', -1):.3f}, LR={probs_t.get('lr', -1):.3f}, "
                      f"RF={probs_t.get('rf', -1):.3f}, GB={probs_t.get('gb',-1):.3f}")
            except NotFittedError:
                print("[Predict SKLEARN] L·ªñI: M√¥ h√¨nh Sklearn ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán.")
            except ValueError as ve:
                 print(f"[Predict SKLEARN] L·ªñI gi√° tr·ªã khi d·ª± ƒëo√°n (c√≥ th·ªÉ do scaler/features): {ve}")
            except Exception as e:
                print(f"[Predict SKLEARN] L·ªñI kh√¥ng x√°c ƒë·ªãnh: {e}")
        else:
             print("[Predict SKLEARN] Kh√¥ng t·∫°o ƒë∆∞·ª£c features.")
    elif not sklearn_models_ready:
         print("[Predict SKLEARN] B·ªè qua - Models ch∆∞a s·∫µn s√†ng.")
    else: # n_hist < sklearn_lookback
         print(f"[Predict SKLEARN] B·ªè qua - L·ªãch s·ª≠ qu√° ng·∫Øn ({n_hist}/{sklearn_lookback}).")

    # --- D·ª± ƒëo√°n t·ª´ LSTM Model ---
    if lstm_model_ready and lstm_model is not None and n_hist >= LSTM_SEQUENCE_LENGTH:
        print("[Predict LSTM] B·∫Øt ƒë·∫ßu...")
        try:
            # Chu·∫©n b·ªã input sequence cu·ªëi c√πng (float32, reshape)
            labels = [1.0 if result == 't' else 0.0 for result in current_history]
            last_sequence = np.array(labels[-LSTM_SEQUENCE_LENGTH:]).astype(np.float32)
            last_sequence = last_sequence.reshape((1, LSTM_SEQUENCE_LENGTH, 1))

            # D·ª± ƒëo√°n x√°c su·∫•t T√†i
            lstm_prob_t = lstm_model.predict(last_sequence, verbose=0)[0][0]
            probs_t['lstm'] = float(lstm_prob_t) # Chuy·ªÉn v·ªÅ float chu·∫©n Python
            print(f"[Predict LSTM] Prob (T): {probs_t['lstm']:.3f}")
        except Exception as e:
            print(f"[Predict LSTM] L·ªñI khi d·ª± ƒëo√°n: {e}")
    elif not lstm_model_ready:
        print("[Predict LSTM] B·ªè qua - Model ch∆∞a s·∫µn s√†ng.")
    else: # n_hist < LSTM_SEQUENCE_LENGTH
        print(f"[Predict LSTM] B·ªè qua - L·ªãch s·ª≠ qu√° ng·∫Øn ({n_hist}/{LSTM_SEQUENCE_LENGTH}).")

    # --- K·∫øt h·ª£p c√°c d·ª± ƒëo√°n (Simple Averaging Ensemble) ---
    valid_probs = [p for p in probs_t.values() if p is not None and 0 <= p <= 1]

    if not valid_probs:
        print("Kh√¥ng c√≥ d·ª± ƒëo√°n h·ª£p l·ªá t·ª´ b·∫•t k·ª≥ m√¥ h√¨nh n√†o. D·ª± ƒëo√°n ng·∫´u nhi√™n.")
        final_prob_t = random.uniform(0.45, 0.55) # H∆°i nhi·ªÖu quanh 0.5
    else:
        final_prob_t = sum(valid_probs) / len(valid_probs)
        print(f"K·∫øt h·ª£p {len(valid_probs)} d·ª± ƒëo√°n h·ª£p l·ªá. Trung b√¨nh Prob (T): {final_prob_t:.4f}")

    # --- ƒê∆∞a ra k·∫øt qu·∫£ cu·ªëi c√πng ---
    final_prediction = "t" if final_prob_t >= 0.50 else "x" # Ng∆∞·ª°ng 0.5
    prob_tai = final_prob_t * 100
    prob_xiu = (1 - final_prob_t) * 100

    print(f"--- D·ª± ƒëo√°n Cu·ªëi c√πng ---")
    print(f"   -> {'T√ÄI' if final_prediction == 't' else 'X·ªàU'}")
    print(f"   -> X√°c su·∫•t T√†i: {prob_tai:.2f}%")
    print(f"   -> X√°c su·∫•t X·ªâu: {prob_xiu:.2f}%")
    print(f"--------------------------")

    return final_prediction, prob_tai, prob_xiu

# --- Ph√°t hi·ªán c·∫ßu (gi·ªØ nguy√™n ho·∫∑c c·∫£i ti·∫øn th√™m n·∫øu mu·ªën) ---
def detect_pattern_v2(history):
    """ Ph√°t hi·ªán c·∫ßu ph·ª©c t·∫°p h∆°n (c√≥ th·ªÉ tham kh·∫£o c√°c di·ªÖn ƒë√†n) """
    n = len(history)
    if n < 5: return "Kh√¥ng ƒë·ªß d·ªØ li·ªáu (<5)"
    recent = history[-15:] # Ph√¢n t√≠ch 15 m·ª•c g·∫ßn nh·∫•t
    m = len(recent)
    s = "".join(recent).upper() # Chu·ªói ƒë·ªÉ t√¨m ki·∫øm

    # ∆Øu ti√™n c√°c c·∫ßu d√†i
    if m>=6 and all(recent[i]==recent[-1] for i in range(-6,0)): return f"B·ªát {recent[-1].upper()} (6+)"
    if m>=8 and all(recent[i]!=recent[i+1] for i in range(-8,-1)): return "1-1 (8+)"
    if m>=8 and (s.endswith("TTXXTTXX") or s.endswith("XXTTXXTT")): return "2-2 (8+)"
    if m>=8 and (s.endswith("TTTXXXTT") or s.endswith("XXXT XXX")): return "3-3 (8+)" # V√≠ d·ª• c·∫ßu 3-3
    if m>=10 and (s.endswith("TXTXTXTXTX") or s.endswith("XTXTXTXTXT")): return "1-1 (10+)"

    # C√°c c·∫ßu ng·∫Øn h∆°n
    if m>=4 and all(recent[i]==recent[-1] for i in range(-4,0)): return f"B·ªát {recent[-1].upper()} (4+)"
    if m>=6 and all(recent[i]!=recent[i+1] for i in range(-6,-1)): return "1-1 (6+)"
    if m>=6 and (s.endswith("TTXXTT") or s.endswith("XXTTXX")): return "2-2 (6+)"
    if m>=5 and (s.endswith("TTTXX") or s.endswith("XXTTT")): return "B·ªát 3 ng·∫Øt 2"
    if m>=5 and (s.endswith("TXT TX") or s.endswith("XTX XT")): return "1-1 ng·∫Øt 1 xen k·∫Ω" # C·∫ßu nh·∫£y

    return "Kh√¥ng r√µ ho·∫∑c c·∫ßu ng·∫Øn"

# --- Hu·∫•n luy·ªán n·ªÅn & L√™n l·ªãch ---
def background_training_job():
    global training_active
    if training_lock.locked():
        print("Hu·∫•n luy·ªán n·ªÅn ƒë√£ ƒëang ch·∫°y, b·ªè qua.")
        return
    print("\n=== Trigger Hu·∫•n luy·ªán N·ªÅn ===")
    # C√≥ th·ªÉ th√™m ƒëi·ªÅu ki·ªán ch·ªâ train n·∫øu c√≥ d·ªØ li·ªáu m·ªõi ƒë·ªß nhi·ªÅu
    train_all_models()

def start_background_training_thread(force=False):
    # Ch·∫°y hu·∫•n luy·ªán trong lu·ªìng ri√™ng
    if not training_active or force:
         print("Kh·ªüi ch·∫°y lu·ªìng hu·∫•n luy·ªán n·ªÅn...")
         training_thread = threading.Thread(target=train_all_models, args=(force,), daemon=True)
         training_thread.start()
    else:
         print("Kh√¥ng b·∫Øt ƒë·∫ßu lu·ªìng hu·∫•n luy·ªán m·ªõi v√¨ ƒëang c√≥ ti·∫øn tr√¨nh ch·∫°y.")

def schedule_background_training(interval_minutes=45):
    """L√™n l·ªãch hu·∫•n luy·ªán ƒë·ªãnh k·ª≥."""
    interval_seconds = interval_minutes * 60
    print(f"B·ªô l√™n l·ªãch hu·∫•n luy·ªán n·ªÅn s·∫Ω ch·∫°y m·ªói {interval_minutes} ph√∫t.")

    def run_scheduler():
        time.sleep(60) # Ch·ªù 1 ph√∫t sau khi bot kh·ªüi ƒë·ªông h·∫≥n
        while True:
            print(f"\n[Scheduler] Ch·ªù {interval_minutes} ph√∫t cho l·∫ßn hu·∫•n luy·ªán n·ªÅn ti·∫øp theo...")
            time.sleep(interval_seconds)
            if not training_active:
                 print("[Scheduler] ƒê√£ ƒë·∫øn gi·ªù, b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán n·ªÅn...")
                 start_background_training_thread()
            else:
                print("[Scheduler] ƒê√£ ƒë·∫øn gi·ªù, nh∆∞ng ƒëang c√≥ hu·∫•n luy·ªán kh√°c ch·∫°y. B·ªè qua.")

    scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
    scheduler_thread.start()


# --- C√°c l·ªánh Bot Telegram ---

async def send_long_message(update: Update, text: str):
    """G·ª≠i tin nh·∫Øn d√†i b·∫±ng c√°ch chia nh·ªè."""
    max_length = constants.MessageLimit.TEXT_LENGTH
    for i in range(0, len(text), max_length):
        await update.message.reply_text(text[i:i + max_length])

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /start - Kh·ªüi ƒë·ªông, t·∫£i d·ªØ li·ªáu, th√¥ng b√°o tr·∫°ng th√°i."""
    load_all_data_models() # T·∫£i m·ªçi th·ª© tr∆∞·ªõc
    user = update.effective_user
    await update.message.reply_text(
        f"ü§ñ **Bot AI T√†i X·ªâu N√¢ng Cao Xin Ch√†o {user.mention_html()}!**\n\n"
        f"T√¥i s·ª≠ d·ª•ng c√°c m√¥ h√¨nh h·ªçc m√°y (bao g·ªìm LSTM) ƒë·ªÉ ph√¢n t√≠ch l·ªãch s·ª≠ v√† d·ª± ƒëo√°n k·∫øt qu·∫£ ti·∫øp theo.\n\n"
        f"üî¢ D·ªØ li·ªáu hi·ªán c√≥: {len(history_data)} records.\n"
        f"‚úÖ Sklearn models: {'S·∫µn s√†ng' if sklearn_models_ready else 'C·∫ßn hu·∫•n luy·ªán'}\n"
        f"üß† LSTM model: {'S·∫µn s√†ng' if lstm_model_ready else 'C·∫ßn hu·∫•n luy·ªán'}\n\n"
        "üö® **L∆ØU √ù QUAN TR·ªåNG:**\n"
        "   - D·ª± ƒëo√°n ch·ªâ mang t√≠nh tham kh·∫£o, kh√¥ng ƒë·∫£m b·∫£o ch√≠nh x√°c 100%.\n"
        "   - Ch∆°i c√≥ tr√°ch nhi·ªám, qu·∫£n l√Ω v·ªën c·∫©n th·∫≠n!\n\n"
        "S·ª≠ d·ª•ng /help ƒë·ªÉ xem danh s√°ch l·ªánh.",
        parse_mode='HTML'
    )
    # T·ª± ƒë·ªông hu·∫•n luy·ªán l·∫ßn ƒë·∫ßu n·∫øu ch∆∞a c√≥ model v√† ƒë·ªß data
    if (not sklearn_models_ready or not lstm_model_ready) and len(history_data) >= MIN_HISTORY_FOR_TRAIN:
         await update.message.reply_text("Ph√°t hi·ªán thi·∫øu model v√† ƒë·ªß d·ªØ li·ªáu. B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán n·ªÅn l·∫ßn ƒë·∫ßu...")
         start_background_training_thread(force=True)

    # Kh·ªüi ƒë·ªông b·ªô l√™n l·ªãch ch·∫°y n·ªÅn (ch·ªâ ch·∫°y 1 l·∫ßn)
    if not hasattr(context.application, '_scheduler_started'):
        schedule_background_training()
        context.application._scheduler_started = True

from telegram.helpers import escape_markdown # Import the helper

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /help - Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n chi ti·∫øt s·ª≠ d·ª•ng MarkdownV2."""

    # Chu·ªói g·ªëc ch∆∞a escape
    help_text_raw = (
        "üìñ *H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng Bot AI T√†i X·ªâu N√¢ng Cao* üìñ\n\n"
        "--- *L·ªánh Ch√≠nh* ---\n"
        "üîπ /predict\n"
        "   D·ª± ƒëo√°n k·∫øt qu·∫£ ti·∫øp theo d·ª±a tr√™n to√†n b·ªô l·ªãch s·ª≠ bot ƒëang ghi nh·ªõ.\n\n"
        "üîπ /tx <l·ªãch s·ª≠ t/x>\n"
        "   D·ª± ƒëo√°n k·∫øt qu·∫£ ti·∫øp theo d·ª±a tr√™n chu·ªói T√†i ('t') X·ªâu ('x') b·∫°n cung c·∫•p.\n"
        "   V√≠ d·ª•: /tx t x t t x x\n"
        "   Quan tr·ªçng: Sau d·ª± ƒëo√°n, bot s·∫Ω h·ªèi b·∫°n k·∫øt qu·∫£ th·ª±c t·∫ø.\n"
        "   => Vi·ªác b·∫°n ph·∫£n h·ªìi ƒê√öNG / SAI gi√∫p bot T·ª∞ H·ªåC v√† c·∫£i thi·ªán!\n\n"
        "--- Qu·∫£n l√Ω D·ªØ li·ªáu ---\n"
        "üîπ /add <l·ªãch s·ª≠ t/x>\n"
        "   Th√™m th·ªß c√¥ng m·ªôt chu·ªói k·∫øt qu·∫£ v√†o b·ªô nh·ªõ c·ªßa bot.\n"
        "   V√≠ d·ª•: /add x t t x\n"
        "   (N√™n d√πng feedback sau /tx thay v√¨ l·ªánh n√†y ƒë·ªÉ ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu)\n\n"
        "üîπ /history [s·ªë l∆∞·ª£ng]\n"
        "   Xem l·ªãch s·ª≠ g·∫ßn ƒë√¢y. M·∫∑c ƒë·ªãnh l√† 30.\n"
        "   V√≠ d·ª• xem 50: /history 50\n\n"
        "--- Th√¥ng tin & Qu·∫£n tr·ªã ---\n"
        "üîπ /status\n"
        "   Ki·ªÉm tra tr·∫°ng th√°i hi·ªán t·∫°i c·ªßa bot (s·ªë l∆∞·ª£ng d·ªØ li·ªáu, models, training).\n\n"
        "üîπ /train\n"
        "   (Qu·∫£n tr·ªã vi√™n) Bu·ªôc bot hu·∫•n luy·ªán l·∫°i t·∫•t c·∫£ m√¥ h√¨nh ngay l·∫≠p t·ª©c v·ªõi d·ªØ li·ªáu hi·ªán t·∫°i. (C√≥ th·ªÉ m·∫•t th·ªùi gian)\n\n"
        "--- Nguy√™n t·∫Øc V√†ng ---\n"
        "   üß† Bot c√†ng c√≥ nhi·ªÅu d·ªØ li·ªáu CH·∫§T L∆Ø·ª¢NG (ƒë∆∞·ª£c x√°c nh·∫≠n qua feedback), d·ª± ƒëo√°n c√†ng c√≥ c∆° s·ªü.\n"
        "   üé∞ Lu√¥n nh·ªõ y·∫øu t·ªë MAY M·∫ÆN trong T√†i X·ªâu.\n"
        "   üí∞ CH∆†I C√ì TR√ÅCH NHI·ªÜM!"
    )

    # Escape cho MarkdownV2
    help_text_md = escape_markdown(help_text_raw, version=2)

    try:
        await update.message.reply_text(help_text_md, parse_mode='MarkdownV2')
    except Exception as e:
        print(f"!!!!!!!! L·ªñI TRONG HELP_COMMAND KHI G·ª¨I MARKDOWNV2: {e}")
        await update.message.reply_text("ƒê√£ x·∫£y ra l·ªói khi hi·ªÉn th·ªã h∆∞·ªõng d·∫´n Markdown. Vui l√≤ng th·ª≠ l·∫°i.")
        
async def predict_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /predict - D·ª± ƒëo√°n d·ª±a tr√™n to√†n b·ªô l·ªãch s·ª≠ hi·ªán c√≥."""
    current_history = list(history_data)
    if len(current_history) < MIN_HISTORY_FOR_PREDICT:
        await update.message.reply_text(f"‚ö†Ô∏è C·∫ßn √≠t nh·∫•t {MIN_HISTORY_FOR_PREDICT} m·ª•c l·ªãch s·ª≠ ƒë·ªÉ d·ª± ƒëo√°n. Hi·ªán c√≥: {len(current_history)}.")
        return

    await update.message.reply_text("‚è≥ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch v√† d·ª± ƒëo√°n d·ª±a tr√™n l·ªãch s·ª≠ hi·ªán t·∫°i...")
    prediction, prob_tai, prob_xiu = combined_prediction(current_history)
    pattern = detect_pattern_v2(current_history) # S·ª≠ d·ª•ng detect_pattern_v2

    await update.message.reply_text(
        f"üîÆ **D·ª± ƒëo√°n Tham Kh·∫£o:**\n\n"
        f"   üß† K·∫øt qu·∫£: **{'T√ÄI' if prediction == 't' else 'X·ªàU'}**\n"
        f"   üìä T·ª∑ l·ªá (T√†i/X·ªâu): {prob_tai:.1f}% / {prob_xiu:.1f}%\n"
        f"   üìà Ph√¢n t√≠ch c·∫ßu: {pattern}\n\n"
        f"*(L∆∞u √Ω: ƒê√¢y l√† d·ª± ƒëo√°n d·ª±a tr√™n to√†n b·ªô l·ªãch s·ª≠ bot c√≥)*"
    )

async def tx(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /tx - D·ª± ƒëo√°n d·ª±a tr√™n l·ªãch s·ª≠ ng∆∞·ªùi d√πng v√† y√™u c·∫ßu feedback."""
    if not context.args:
        await update.message.reply_text("‚ö†Ô∏è Vui l√≤ng cung c·∫•p l·ªãch s·ª≠! V√≠ d·ª•: `/tx t x t x x`")
        return

    user_input_str = "".join(context.args).lower()
    # L·ªçc ch·ªâ gi·ªØ l·∫°i 't' v√† 'x'
    user_provided_history = [char for char in user_input_str if char in ['t', 'x']]

    if not user_provided_history:
        await update.message.reply_text("‚ö†Ô∏è L·ªãch s·ª≠ kh√¥ng h·ª£p l·ªá. Ch·ªâ d√πng 't' ho·∫∑c 'x'. V√≠ d·ª•: `/tx t x t x x`")
        return

    if len(user_provided_history) < 5:
        await update.message.reply_text("‚ö†Ô∏è L·ªãch s·ª≠ cung c·∫•p qu√° ng·∫Øn (<5). N√™n cung c·∫•p √≠t nh·∫•t 5-10 k·∫øt qu·∫£ g·∫ßn nh·∫•t.")
        # return # Cho ph√©p ch·∫°y v·ªõi ls ng·∫Øn nh∆∞ng c·∫£nh b√°o

    print(f"Nh·∫≠n l·ªánh /tx v·ªõi l·ªãch s·ª≠: {''.join(user_provided_history)}")
    await update.message.reply_text(f"‚è≥ ƒêang ph√¢n t√≠ch l·ªãch s·ª≠ b·∫°n cung c·∫•p: `{''.join(user_provided_history)}`...", parse_mode='Markdown')

    # T·∫°o l·ªãch s·ª≠ t·∫°m th·ªùi ƒë·ªÉ d·ª± ƒëo√°n = l·ªãch s·ª≠ bot + l·ªãch s·ª≠ ng∆∞·ªùi d√πng
    # Ch·ªâ l·∫•y ph·∫ßn ƒë·ªß d√πng ƒë·ªÉ tr√°nh qu√° d√†i
    combined_hist_for_predict = list(history_data)[-HISTORY_MAXLEN + len(user_provided_history):] + user_provided_history
    if len(combined_hist_for_predict) > HISTORY_MAXLEN:
         combined_hist_for_predict = combined_hist_for_predict[-HISTORY_MAXLEN:]

    prediction, prob_tai, prob_xiu = combined_prediction(combined_hist_for_predict)
    pattern = detect_pattern_v2(combined_hist_for_predict)

    # --- T·∫°o Callback Data Quan Tr·ªçng ---
    # L∆∞u l·∫°i l·ªãch s·ª≠ user cung c·∫•p v√† d·ª± ƒëo√°n c·ªßa bot
    # Format: "txfeedback | user_history_str | prediction"
    user_history_str_for_callback = "".join(user_provided_history)
    # Gi·ªõi h·∫°n ƒë·ªô d√†i callback data (Telegram gi·ªõi h·∫°n 64 bytes)
    max_hist_len_callback = 30 # Gi·ªõi h·∫°n l·ªãch s·ª≠ trong callback
    if len(user_history_str_for_callback) > max_hist_len_callback:
        user_history_str_for_callback = user_history_str_for_callback[-max_hist_len_callback:]

    callback_data_prefix = f"txf|{user_history_str_for_callback}|{prediction}" # txf = tx feedback

    # Ki·ªÉm tra ƒë·ªô d√†i callback data
    if len(callback_data_prefix.encode('utf-8')) > 60: # ƒê·ªÉ d∆∞ v√†i byte cho '|correct/wrong'
         # C·∫ßn gi·∫£m ƒë·ªô d√†i user_history_str_for_callback h∆°n n·ªØa
         estimated_overhead = len("txf||c") # Overhead kho·∫£ng n√†y
         max_allowed_hist_len = 64 - estimated_overhead - len(prediction)
         user_history_str_for_callback = user_history_str_for_callback[-max_allowed_hist_len:]
         callback_data_prefix = f"txf|{user_history_str_for_callback}|{prediction}"
         print(f"WARN: C·∫Øt ng·∫Øn callback history xu·ªëng c√≤n {len(user_history_str_for_callback)}")

    buttons = InlineKeyboardMarkup([
        [
            InlineKeyboardButton(f"‚úÖ Th·ª±c t·∫ø l√† {prediction.upper()}", callback_data=f"{callback_data_prefix}|correct"),
            InlineKeyboardButton(f"‚ùå Kh√¥ng ph·∫£i {prediction.upper()}", callback_data=f"{callback_data_prefix}|wrong")
        ]
        # C√≥ th·ªÉ th√™m n√∫t "B·ªè qua" n·∫øu mu·ªën
        # [InlineKeyboardButton("ü§∑ B·ªè qua", callback_data="txf_ignore")]
    ])

    await update.message.reply_text(
        f"üìä **D·ª± ƒêo√°n Cho L·ªãch S·ª≠ B·∫°n Cung C·∫•p:**\n\n"
        f"   üëâ `{user_provided_history}` => D·ª± ƒëo√°n: **{'T√ÄI' if prediction == 't' else 'X·ªàU'}**\n"
        f"   üéØ T·ª∑ l·ªá (T/X): {prob_tai:.1f}% / {prob_xiu:.1f}%\n"
        f"   üìà C·∫ßu (ph√¢n t√≠ch t·∫°m th·ªùi): {pattern}\n\n"
        f"‚ùì **K·∫øt qu·∫£ th·ª±c t·∫ø c·ªßa phi√™n ƒë√≥ l√† g√¨?**\n"
        f"   *(Ch·ªçn 1 n√∫t b√™n d∆∞·ªõi ƒë·ªÉ gi√∫p bot h·ªçc h·ªèi)*",
        reply_markup=buttons,
        parse_mode='Markdown'
    )


async def add(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /add - Th√™m d·ªØ li·ªáu th·ªß c√¥ng (√≠t khuy·∫øn kh√≠ch)."""
    if not context.args:
        await update.message.reply_text("‚ö†Ô∏è Vui l√≤ng cung c·∫•p l·ªãch s·ª≠ t/x. V√≠ d·ª•: `/add t x x t`")
        return

    input_str = "".join(context.args).lower()
    history_to_add = [char for char in input_str if char in ['t', 'x']]

    if history_to_add:
        # Th√™m v√†o b√™n ph·∫£i deque
        added_count = 0
        for item in history_to_add:
            if len(history_data) < HISTORY_MAXLEN:
                history_data.append(item)
                added_count += 1
            else: # N·∫øu deque ƒë√£ ƒë·∫ßy, b·ªè c√°i c≈© nh·∫•t v√† th√™m c√°i m·ªõi
                history_data.append(item) # Deque t·ª± x·ª≠ l√Ω vi·ªác b·ªè b√™n tr√°i
                added_count += 1 # V·∫´n t√≠nh l√† ƒë√£ th√™m
        await update.message.reply_text(f"‚úÖ ƒê√£ th√™m {added_count} k·∫øt qu·∫£ v√†o l·ªãch s·ª≠. T·ªïng s·ªë: {len(history_data)}.")
        # L∆∞u l·∫°i ngay sau khi th√™m th·ªß c√¥ng
        save_all_data_models()
        # C√¢n nh·∫Øc hu·∫•n luy·ªán n·∫øu th√™m nhi·ªÅu? (c√≥ th·ªÉ g√¢y t·ªën t√†i nguy√™n)
        # if added_count > 10 and not training_active:
        #     start_background_training_thread()
    else:
        await update.message.reply_text("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu T√†i/X·ªâu (t/x) h·ª£p l·ªá ƒë·ªÉ th√™m.")


async def history_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /history - Hi·ªÉn th·ªã l·ªãch s·ª≠."""
    count = 30 # M·∫∑c ƒë·ªãnh
    if context.args:
        try:
            count = int(context.args[0])
            if not 1 <= count <= 200: # Gi·ªõi h·∫°n xem t·ªëi ƒëa 200
                await update.message.reply_text("‚ö†Ô∏è S·ªë l∆∞·ª£ng xem ph·∫£i t·ª´ 1 ƒë·∫øn 200.")
                return
        except (ValueError, IndexError):
            await update.message.reply_text("‚ö†Ô∏è S·ªë l∆∞·ª£ng kh√¥ng h·ª£p l·ªá. V√≠ d·ª•: `/history 50`")
            return

    history_list = list(history_data)
    if not history_list:
        await update.message.reply_text("‚õî L·ªãch s·ª≠ ƒëang tr·ªëng.")
        return

    display_count = min(count, len(history_list))
    recent_history = history_list[-display_count:]
    history_str = " ".join(item.upper() for item in recent_history) # Vi·∫øt hoa T X

    msg = (
        f"üìú **L·ªãch s·ª≠ {display_count} K·∫øt Qu·∫£ G·∫ßn Nh·∫•t** (T·ªïng: {len(history_list)}):\n\n"
        f"`{history_str}`"
    )
    await update.message.reply_text(msg, parse_mode='Markdown')


async def status_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /status - Ki·ªÉm tra tr·∫°ng th√°i chi ti·∫øt."""
    status_msg = (
        f"üìä **Tr·∫°ng Th√°i Bot AI T√†i X·ªâu** üìä\n\n"
        f"**D·ªØ Li·ªáu:**\n"
        f"  - L·ªãch s·ª≠ T/X: {len(history_data)} / {HISTORY_MAXLEN}\n"
        # f"  - D·ªØ li·ªáu S√∫c s·∫Øc: {len(dice_data)} / {DICE_MAXLEN}\n"
        f"\n**M√¥ H√¨nh:**\n"
        f"  - Sklearn (NB, LR, RF, GB): {'‚úÖ S·∫µn s√†ng' if sklearn_models_ready else '‚ùå Ch∆∞a hu·∫•n luy·ªán / L·ªói'}\n"
        f"  - Deep Learning (LSTM): {'üß† S·∫µn s√†ng' if lstm_model_ready else '‚ùå Ch∆∞a hu·∫•n luy·ªán / L·ªói'}\n"
        f"\n**Hu·∫•n Luy·ªán:**\n"
        f"  - ƒêang hu·∫•n luy·ªán n·ªÅn: {'‚è≥ C√≥' if training_active else 'üö´ Kh√¥ng'}\n"
        f"\n*(C√°c m√¥ h√¨nh ƒë∆∞·ª£c t·ª± ƒë·ªông hu·∫•n luy·ªán l·∫°i ƒë·ªãnh k·ª≥ ho·∫∑c khi c√≥ ƒë·ªß d·ªØ li·ªáu m·ªõi ƒë∆∞·ª£c x√°c nh·∫≠n)*"
    )
    await update.message.reply_text(status_msg)

async def train_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /train - Bu·ªôc hu·∫•n luy·ªán l·∫°i (ch·ªâ admin?)."""
    # Th√™m ki·ªÉm tra quy·ªÅn admin n·∫øu c·∫ßn
    # admin_ids = [123456789] # Th√™m ID c·ªßa b·∫°n v√†o ƒë√¢y
    # if update.effective_user.id not in admin_ids:
    #    await update.message.reply_text("‚õî B·∫°n kh√¥ng c√≥ quy·ªÅn th·ª±c hi·ªán l·ªánh n√†y.")
    #    return

    if training_active:
        await update.message.reply_text("‚è≥ Qu√° tr√¨nh hu·∫•n luy·ªán kh√°c ƒëang ch·∫°y. Vui l√≤ng ƒë·ª£i...")
        return

    await update.message.reply_text("‚öôÔ∏è ƒê√£ nh·∫≠n l·ªánh! B·∫Øt ƒë·∫ßu **bu·ªôc** hu·∫•n luy·ªán l·∫°i t·∫•t c·∫£ m√¥ h√¨nh...")
    start_background_training_thread(force=True) # S·ª≠ d·ª•ng c·ªù force
    await update.message.reply_text("‚úÖ ƒê√£ kh·ªüi ch·∫°y hu·∫•n luy·ªán n·ªÅn. Theo d√µi log ho·∫∑c d√πng /status ƒë·ªÉ ki·ªÉm tra ti·∫øn tr√¨nh.")


# --- X·ª≠ l√Ω Callback (Quan tr·ªçng cho T·ª± h·ªçc) ---
async def handle_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """X·ª≠ l√Ω feedback t·ª´ n√∫t b·∫•m sau l·ªánh /tx."""
    query = update.callback_query
    await query.answer() # Th√¥ng b√°o ƒë√£ nh·∫≠n

    # Format data: "txf | user_history_str | prediction | result(correct/wrong)"
    try:
        prefix, hist_str, prediction, result = query.data.split("|")

        if prefix == "txf":
            print(f"Callback Feedback: History='{hist_str}', Prediction='{prediction}', Result='{result}'")

            if result == "correct":
                # K·∫øt qu·∫£ ƒë√∫ng -> L·ªãch s·ª≠ ng∆∞·ªùi d√πng cung c·∫•p l√† ch√≠nh x√°c V√Ä k·∫øt qu·∫£ SAU ƒë√≥ l√† d·ª± ƒëo√°n c·ªßa bot
                validated_history = list(hist_str) # ['t', 'x', 't']
                actual_outcome = prediction         # 'x'

                # Th√™m l·ªãch s·ª≠ g·ªëc v√† k·∫øt qu·∫£ th·ª±c t·∫ø v√†o data ch√≠nh
                history_data.extend(validated_history)
                history_data.append(actual_outcome)
                added_count = len(validated_history) + 1

                await query.edit_message_text(
                    f"‚úÖ C·∫£m ∆°n b·∫°n ƒë√£ x√°c nh·∫≠n!\n"
                    f"ƒê√£ th√™m {added_count} k·∫øt qu·∫£ (`{hist_str} -> {actual_outcome.upper()}`) v√†o b·ªô nh·ªõ ƒë·ªÉ bot h·ªçc h·ªèi.",
                    parse_mode='Markdown'
                )
                save_all_data_models() # L∆∞u ngay l·∫≠p t·ª©c
                # C√≥ th·ªÉ trigger hu·∫•n luy·ªán n·∫øu c√≥ ƒë·ªß d·ªØ li·ªáu m·ªõi
                # if not training_active and len(history_data) % 20 == 0: # V√≠ d·ª• train l·∫°i m·ªói 20 records m·ªõi
                #     start_background_training_thread()

            elif result == "wrong":
                # K·∫øt qu·∫£ sai -> L·ªãch s·ª≠ ng∆∞·ªùi d√πng cung c·∫•p l√† ch√≠nh x√°c, NH∆ØNG k·∫øt qu·∫£ SAU ƒë√≥ KH√ÅC v·ªõi d·ª± ƒëo√°n
                validated_history = list(hist_str)
                actual_outcome = 'x' if prediction == 't' else 't' # K·∫øt qu·∫£ ng∆∞·ª£c l·∫°i

                # Th√™m l·ªãch s·ª≠ g·ªëc v√† k·∫øt qu·∫£ th·ª±c t·∫ø (ƒë√£ s·ª≠a) v√†o data
                history_data.extend(validated_history)
                history_data.append(actual_outcome)
                added_count = len(validated_history) + 1

                await query.edit_message_text(
                     f"‚úÖ C·∫£m ∆°n b·∫°n ƒë√£ ph·∫£n h·ªìi!\n"
                     f"ƒê√£ ghi nh·∫≠n k·∫øt qu·∫£ th·ª±c t·∫ø l√† `{actual_outcome.upper()}` (kh√°c d·ª± ƒëo√°n) v√† th√™m {added_count} records (`{hist_str} -> {actual_outcome.upper()}`) v√†o b·ªô nh·ªõ.",
                     parse_mode='Markdown'
                 )
                save_all_data_models()
                # if not training_active and len(history_data) % 20 == 0:
                #     start_background_training_thread()

        # elif prefix == "txf_ignore":
        #     await query.edit_message_text("‚ÑπÔ∏è ƒê√£ b·ªè qua ph·∫£n h·ªìi cho d·ª± ƒëo√°n n√†y.")

        else:
            await query.edit_message_text("L·ªói: H√†nh ƒë·ªông callback kh√¥ng h·ª£p l·ªá.")
            print(f"L·ªói Callback: Prefix kh√¥ng ƒë√∫ng - {query.data}")

    except ValueError:
         print(f"L·ªói Callback: Kh√¥ng th·ªÉ split data - {query.data}")
         await query.edit_message_text("L·ªói x·ª≠ l√Ω ph·∫£n h·ªìi (data format sai).")
    except Exception as e:
        print(f"L·ªói Callback Nghi√™m tr·ªçng: {e} \nData: {query.data}")
        try:
            await query.edit_message_text("ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën khi x·ª≠ l√Ω l·ª±a ch·ªçn c·ªßa b·∫°n.")
        except Exception: pass # B·ªè qua n·∫øu kh√¥ng g·ª≠i ƒë∆∞·ª£c tin nh·∫Øn l·ªói

async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Log c√°c l·ªói v√† g·ª≠i tin nh·∫Øn b√°o l·ªói n·∫øu c√≥ th·ªÉ."""
    print(f"Exception while handling an update: {context.error}")
    # traceback.print_exception(type(context.error), context.error, context.error.__traceback__) # Log chi ti·∫øt h∆°n n·∫øu c·∫ßn

    # Th·ª≠ g·ª≠i tin nh·∫Øn l·ªói cho ng∆∞·ªùi d√πng (c√≥ th·ªÉ th·∫•t b·∫°i n·∫øu l·ªói m·∫°ng)
    if update and hasattr(update, 'effective_message') and update.effective_message:
        try:
            await update.effective_message.reply_text("‚ùóÔ∏è ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i sau.")
        except Exception as e:
            print(f"Kh√¥ng th·ªÉ g·ª≠i tin nh·∫Øn l·ªói cho ng∆∞·ªùi d√πng: {e}")

# --- Main Execution ---
if __name__ == "__main__":
    print("--- Kh·ªüi t·∫°o Bot AI T√†i X·ªâu N√¢ng Cao ---")
    application = ApplicationBuilder().token(TOKEN).build()

    # Th√™m c√°c handlers
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("predict", predict_command))
    application.add_handler(CommandHandler("tx", tx))
    application.add_handler(CommandHandler("add", add))
    application.add_handler(CommandHandler("history", history_command))
    application.add_handler(CommandHandler("status", status_command))
    application.add_handler(CommandHandler("train", train_command))
    application.add_handler(CallbackQueryHandler(handle_callback)) # X·ª≠ l√Ω n√∫t b·∫•m quan tr·ªçng
    application.add_error_handler(error_handler) # X·ª≠ l√Ω l·ªói chung

    print("\n--- Bot ƒë√£ s·∫µn s√†ng l·∫Øng nghe ---")
    # Ch·∫°y bot non-blocking ƒë·ªÉ cho ph√©p c√°c ti·∫øn tr√¨nh n·ªÅn (scheduler) ch·∫°y
    application.run_polling(allowed_updates=Update.ALL_TYPES)

    # Ph·∫ßn n√†y s·∫Ω kh√¥ng bao gi·ªù ƒë·∫°t ƒë∆∞·ª£c n·∫øu run_polling ch·∫°y m√£i m√£i
    print("--- Bot ƒëang d·ª´ng ---")